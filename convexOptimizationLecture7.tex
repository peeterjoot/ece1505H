%
% Copyright © 2017 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%
\input{../latex/blogpost.tex}
\renewcommand{\basename}{convexOptimization7}
\renewcommand{\dirname}{notes/ece1505/}
\newcommand{\keywords}{ECE1505H}
\input{../latex/peeter_prologue_print2.tex}

\usepackage{ece1505}
\usepackage{peeters_braket}
%\usepackage{peeters_layout_exercise}
\usepackage{peeters_figures}
\usepackage{mathtools}
\usepackage{siunitx}

\beginArtNoToc
\generatetitle{ECE1505H Convex Optimization.  Lecture 7: XXX.  Taught by Prof.\ Stark Draper}
%\chapter{XXX}
\label{chap:convexOptimization7}

\paragraph{Disclaimer}

Peeter's lecture notes from class.  These may be incoherent and rough.

These are notes for the UofT course ECE1505H, Convex Optimization, taught by Prof. Stark Draper, covering \textchapref{{1}} \citep{boyd2004convex} content.

\paragraph{Today}

\begin{itemize}
\item Local and global optimality
\item Compositions of functions
\item Examples
\end{itemize}

Example

\begin{dmath}\label{eqn:convexOptimizationLecture7:20}
\begin{aligned}
F(x) &= x^2  \\
F''(x) &= 2 > 0
\end{aligned}
\end{dmath}

strictly convex.

Example

\begin{dmath}\label{eqn:convexOptimizationLecture7:40}
\begin{aligned}
F(x) &= x^3  \\
F''(x) &= 6 x.
\end{aligned}
\end{dmath}

Not always non-negative, so not convex.  However \( x^3 \) is convex on \( \dom F = \bbR_{+} \).

Example:

\begin{dmath}\label{eqn:convexOptimizationLecture7:60}
\begin{aligned}
F(x) &= x^\alpha \\
F'(x) &= \alpha x^{\alpha-1} \\
F''(x) &= \alpha(\alpha-1) x^{\alpha-2}.
\end{aligned}
\end{dmath}

F1

This is convex on \( \bbR_{+} \), if \( \alpha \ge 1 \), or \( \alpha \le 0 \).

Example:

\begin{dmath}\label{eqn:convexOptimizationLecture7:80}
\begin{aligned}
F(x) &= \log x \\
F'(x) &= \inv{x} \\
F''(x) &= -\inv{x^2} \le 0
\end{aligned}
\end{dmath}

This is concave.

Example:

\begin{dmath}\label{eqn:convexOptimizationLecture7:100}
\begin{aligned}
F(x) &= x\log x \\
F'(x) &= \log x + x \inv{x} = 1 + \log x \\
F''(x) &= \inv{x}
\end{aligned}
\end{dmath}

This is strictly convext on
\( \bbR_{++} \), where
\( F''(x) \ge 0 \).

Example

\begin{dmath}\label{eqn:convexOptimizationLecture7:120}
\begin{aligned}
F(x) &= e^{\alpha x} \\
F'(x) &= \alpha e^{\alpha x} \\
F''(x) &= \alpha^2 e^{\alpha x} \ge 0
\end{aligned}
\end{dmath}

F2

This is a convex function for all \( \alpha \).

Example:

For symmetric \( P \in S^n \)

\begin{dmath}\label{eqn:convexOptimizationLecture7:140}
\begin{aligned}
F(\Bx) &= \Bx^\T P \Bx + 2 \Bq^\T \Bx + r \\
\spacegrad F &= (P + P^\T) \Bx + 2 \Bq = 2 P \Bx + 2 \Bq \\
\spacegrad^2 F &= 2 P.
\end{aligned}
\end{dmath}

This is convex(concave) if \( P \ge 0 \) (\( P \le 0\)).

Example:

For
\begin{dmath}\label{eqn:convexOptimizationLecture7:160}
F(x, y) = x^2 + y^2 + 3 x y
=
\begin{bmatrix}
x & y
\end{bmatrix}
\begin{bmatrix}
1 & 1.5 \\
1.5 & 1
\end{bmatrix}
\begin{bmatrix}
x \\
 y
\end{bmatrix}
\end{dmath}

\begin{dmath}\label{eqn:convexOptimizationLecture7:180}
\spacegrad^2 F
=
\begin{bmatrix}
\partial_{xx} F & \partial_{xy} F \\
\partial_{yx} F & \partial_{yy} F \\
\end{bmatrix}
=
\begin{bmatrix}
2 & 3 \\
3 & 2
\end{bmatrix}
= 2 P.
\end{dmath}

Is this PSD?  Check eigenvalues

\begin{dmath}\label{eqn:convexOptimizationLecture7:200}
0 =
\det ( P - \lambda I )
=
(1 - \lambda)^2 - 1.5^2,
\end{dmath}

which has solutions

\begin{dmath}\label{eqn:convexOptimizationLecture7:220}
\lambda = 1 \pm \frac{3}{2} = \frac{3}{2}, -\frac{1}{2}.
\end{dmath}

This is not PSD nor negative semi-definite, because it has one positive and one negative eigenvalues.  This is neither convex nor concave.

Along \( y = -x \),

\begin{dmath}\label{eqn:convexOptimizationLecture7:240}
F(x,y)
=
F(x,-x)
=
2 x^2 - 3 x^2
=
- x^2,
\end{dmath}

so it is concave along this line.  Along \( y = x \)

\begin{dmath}\label{eqn:convexOptimizationLecture7:260}
F(x,y)
=
F(x,x)
=
2 x^2 + 3 x^2
=
5 x^2,
\end{dmath}

so it is convex along this line.

Example:

\begin{dmath}\label{eqn:convexOptimizationLecture7:280}
F(\Bx) = \sqrt{ x_1 x_2 },
\end{dmath}

on \( \dom F = \setlr{ x_1 \ge 0, x_2 \ge 0 } \)

For the Hessian
\begin{dmath}\label{eqn:convexOptimizationLecture7:300}
\begin{aligned}
\PD{x_1}{F} &= \frac{1}{2} x_1^{-1/2} x_2^{1/2} \\
\PD{x_2}{F} &= \frac{1}{2} x_2^{-1/2} x_1^{1/2}
\end{aligned}
\end{dmath}

The Hessian components are

\begin{dmath}\label{eqn:convexOptimizationLecture7:320}
\begin{aligned}
\PD{x_1}{} \PD{x_1}{F} &= -\frac{1}{4} x_1^{-3/2} x_2^{1/2} \\
\PD{x_1}{} \PD{x_2}{F} &= \frac{1}{4} x_2^{-1/2} x_1^{-1/2} \\
\PD{x_2}{} \PD{x_1}{F} &= \frac{1}{4} x_1^{-1/2} x_2^{-1/2} \\
\PD{x_2}{} \PD{x_2}{F} &= -\frac{1}{4} x_2^{-3/2} x_1^{1/2}
\end{aligned}
\end{dmath}

or
\begin{dmath}\label{eqn:convexOptimizationLecture7:340}
\spacegrad^2 F
=
-\frac{\sqrt{x_1 x_2}}{4}
\begin{bmatrix}
\inv{x_1^2} & -\inv{x_1 x_2} \\
-\inv{x_1 x_2} & \inv{x_2^2}
\end{bmatrix}.
\end{dmath}

Checking this for PSD against \( \Bv = (v_1, v_2) \), we have
\begin{dmath}\label{eqn:convexOptimizationLecture7:360}
\begin{bmatrix}
v_1 & v_2
\end{bmatrix}
\begin{bmatrix}
\inv{x_1^2} & -\inv{x_1 x_2} \\
-\inv{x_1 x_2} & \inv{x_2^2}
\end{bmatrix}
\begin{bmatrix}
v_1 \\ v_2
\end{bmatrix}
=
\begin{bmatrix}
v_1 & v_2
\end{bmatrix}
\begin{bmatrix}
\inv{x_1^2} v_1 -\inv{x_1 x_2} v_2  \\
-\inv{x_1 x_2} v_1 + \inv{x_2^2} v_2
\end{bmatrix}
=
\lr{ \inv{x_1^2} v_1 -\inv{x_1 x_2} v_2 } v_1 +
\lr{ -\inv{x_1 x_2} v_1 + \inv{x_2^2} v_2 } v_2
=
\lr{
\frac{v_1}{x_1}
-\frac{v_2}{x_2}
}^2
\ge 0,
\end{dmath}

so \( \spacegrad^2 F \le 0 \).  This is a negative semi-definite function (concave).  Observe that this check required checking PSD for all values of \( \Bx \).

This is an example of a more general result

\begin{dmath}\label{eqn:convexOptimizationLecture7:n}
F(x) = \lr{ \prod_{i = 1}^n x_i }^{1/n},
\end{dmath}

which is concave (prove on homework).

\paragraph{summary}

If \( F \) is differentiable in \R{n}, then check the curvature of the function along all lines.  i.e.  At all locations and in all directions.

If the Hessian is PSD at all \( \Bx \in \dom F \), that is

\begin{dmath}\label{eqn:convexOptimizationLecture7:n}
\spacegrad^2 F \ge 0 \, \forall \Bx \in \dom F,
\end{dmath}

then the function is convex.

\paragraph{more examples of convex, but not nessessarily differentiable functions}

Example:
Over \( \dom F = \bbR^n \)

\begin{dmath}\label{eqn:convexOptimizationLecture7:n}
F(\Bx) = \max_{i = 1}^n x_i
\end{dmath}

i.e.
\begin{dmath}\label{eqn:convexOptimizationLecture7:n}
\begin{aligned}
F((1,2) &= 2 \\
F((3,-1) &= 3
\end{aligned}
\end{dmath}

Example:

\begin{dmath}\label{eqn:convexOptimizationLecture7:n}
F(\Bx) = \max_{i = 1}^n F_i(\Bx),
\end{dmath}

where 

\begin{dmath}\label{eqn:convexOptimizationLecture7:n}
F_i(\Bx)
=
... ?
\end{dmath}

max of a set of convex functions is a convex function.

Example:

\begin{dmath}\label{eqn:convexOptimizationLecture7:n}
F(x) = 
x_{[1]} + 
x_{[2]} + 
x_{[3]}
\end{dmath}

where 

\( x_{[k]} \) is the k-th largest number in the list

Write

\begin{dmath}\label{eqn:convexOptimizationLecture7:n}
F(x) = \max x_i + x_j + x_k
\end{dmath}

\begin{dmath}\label{eqn:convexOptimizationLecture7:n}
(i,j,k) \in \choose{n}{3}
\end{dmath}

Example:

For \( \Ba \in \bbR^n \) and \( b_i \in \bbR \)

\begin{dmath}\label{eqn:convexOptimizationLecture7:n}
F(\Bx) 
= \sum_{i = 1}^n \log( b_i - \Ba^\T \Bx )^{-1}
= -\sum_{i = 1}^n \log( b_i - \Ba^\T \Bx )
\end{dmath}

This \( b_i - \Ba^\T \Bx \) is an affine function of \( \Bx \) so it doesn't affect convexity.

Since \( \log \) is concave, \( -\log \) is convex.  Convex functions of affine function of \( \Bx \) is convex function of \( \Bx \).

Example:
\begin{dmath}\label{eqn:convexOptimizationLecture7:n}
F(\Bx) = \sup_{\By \in C} \Norm{ \Bx -  \By }
\end{dmath}

F3

Here \( C \subseteq \bbR^n \) is not neccessarily convex.  We are using \( \sup \) here because the set \( C \) may be open.  This function is the length of the line from \( \Bx \) to the point in \( C \) that is furthest from \( \Bx \).

\begin{itemize}
\item \( \Bx - \By \) is linear in \( \Bx \)
\item \( g_\By(\Bx) = \Norm{\Bx - \By} \) is convex in \( \Bx \) since norms are convex functions.
\item \( F(\Bx) = \sup_{\By \in C} \Norm{ \Bx -  \By } \).  Each \( \By \) index is a convex function.  Taking max of those.
\item
\end{itemize}

Example:

\begin{dmath}\label{eqn:convexOptimizationLecture7:n}
F(\Bx) = \inf_{\By \in C} \Norm{ \Bx -  \By }.
\end{dmath}

F4

F5 : % F(\Bz) = F(\theta \Bx + (1-\theta) \By) \ge \theta F(\Bx) + (1-\theta)F(\By)

This is not neccessarily convex for all sets \( C \subseteq \bbR^n \), because the \( \inf \) of a bunch of convex function is not ncessarily convex.  However, if \( C \) is convex, then \( F(\Bx) \) is convex.

\paragraph{Consequences of convexity for differentiable functions}

\begin{itemize}
\item Think about unconstrained functions \( \dom F = \bbR^n \).
\item By first order condition \( F \) is convex iff the domain is convex and 
\begin{dmath}\label{eqn:convexOptimizationLecture7:n}
F(\Bx) \ge \lr{ \spacegrad F(\Bx)}^\T (\By - \Bx) \, \forall \Bx, \By \in \dom F.
\end{dmath}
\end{itemize}

If \( F \) is convex and one can find an \( \Bx^\conj \in \dom F \) such that 

\begin{dmath}\label{eqn:convexOptimizationLecture7:n}
\spacegrad F(\Bx^\conj) = 0,
\end{dmath}

then

\begin{dmath}\label{eqn:convexOptimizationLecture7:n}
F(\By) \ge F(\Bx^\conj) \, \forall \By \in \dom F.
\end{dmath}

If you can find the point where the gradient is zero (which can't always be found), then \( \Bx^\conj\) is a global minimum of \( F \).

Conversely, if \( \Bx^\conj \) is a global minimizer of \( F \), then \( \spacegrad F(\Bx^\conj) = 0 \) must hold.  If that were not the case, then you would be able to find a direction to move downhill, contracting the optimality of \( \Bx^\conj\).

\paragraph{Local vs Global optimum}

\makedefinition{Local optimum.}{dfn:convexOptimizationLecture7:n}{
\( \Bx^\conj \) is a local optimum of \( F \) if \( \exists \epsilon > 0 \) such that \( \forall \Bx \), \( \Norm{\Bx - \Bx^\conj} < \epsilon \), we have 

\begin{equation*}
F(\Bx^\conj\) \le F(\Bx)
\end{equation*}
} % definition

F6

\maketheorem{}{thm:convexOptimizationLecture7:n}{
Suppose \( F \) is twice continuously differentiable (not neccessarily convex) 

\begin{itemize}
\item 
If \( \Bx^\conj\) is a local optimimum then

\begin{equation*}
\begin{aligned}
\spacegrad F(\Bx^\conj) &= 0 \\
\spacegrad^2 F(\Bx^\conj) \ge 0
\end{aligned}
\end{equation*}

\item
If
\begin{equation*}
\begin{aligned}
\spacegrad F(\Bx^\conj) &= 0 \\
\spacegrad^2 F(\Bx^\conj) \ge 0
\end{aligned},
\end{equation*}

then \( \Bx^\conj\) is a local optimimum.
\end{itemize}
} % theorem

Proof:

\begin{itemize}
\item Let \( \Bx^\conj \) be a local optimum.  Pick any \( \Bv \in \bbR^n \).

\begin{dmath}\label{eqn:convexOptimizationLecture7:n}
\lim_{t \rightarrow 0} \frac{ F(\Bx^\conj + t \Bv) - F(\Bx^\conj)}{t} 
= \lr{ \spacegrad F(\Bx^\conj) }^\T \Bv
\ge 0.
\end{dmath}
\end{itemize}

Here the fraction is \( \ge 0 \) since \( \Bx^\conj \) is a local optimum.

Since the choice of \( \Bv \) is arbitrary, the only case that you can ensure that \( \ge 0, \forall \Bv \) is 

\begin{dmath}\label{eqn:convexOptimizationLecture7:n}
\spacegrad F = 0,
\end{dmath}

( or else could pick \( \Bv = -\spacegrad F(\Bx^\conj) \).

This means that \( \spacegrad F(\Bx^\conj) = 0 \) if \( \Bx^\conj \) is a local optimum.

Consider the 2nd order derivative

\begin{dmath}\label{eqn:convexOptimizationLecture7:n}
\lim_{t \rightarrow 0} \frac{ F(\Bx^\conj + t \Bv) - F(\Bx^\conj)}{t^2} 
=
\lim_{t \rightarrow 0} \inv{t^2} 
\lr{ 
F(\Bx^\conj) + t \lr{ \spacegrad F(\Bx^\conj) }^\T \Bv + \inv{2} t^2 \Bv^\T \spacegrad^2 F(\Bx^\conj) \Bv + O(t^3)
- F(\Bx^\conj)
}
=
\inv{2} \Bv^\T \spacegrad^2 F(\Bx^\conj) \Bv
\ge 0.
\end{dmath}

Here the \( \ge \) condition also comes from the fraction, based on the optimiality of \( \Bx^\conj \).  This is true for all choice of \( \Bv \), thus \( \spacegrad^2 F(\Bx^\conj) \).

% handout:
%Ps1: 1.6(b) plot solution is correct, but rest is for [3 0, 0, 3] not the problem specified.

\EndArticle
%\EndNoBibArticle
