%
% Copyright © 2017 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%
\makeproblem{Ellipses, eigenvalues, eigenvectors, and volume}{convex-optimization:problemSet1:6}{
Make neat and clearly-labelled sketches of the ellipsoid \( \calE = \setlr{\Bx | (\Bx - \Bx_c)^\T P^{-1} (\Bx - \Bx_c) = 1} \) for the following sets of parameters:
\makesubproblem{}{convex-optimization:problemSet1:6a}

Center \( \Bx_c =
\begin{bmatrix}
0 \\ 0
\end{bmatrix} \)
and \( P =
\begin{bmatrix}
1.5 & -0.5 \\
-0.5 & 1.5
\end{bmatrix} \).

\makesubproblem{}{convex-optimization:problemSet1:6b}
Center \( \Bx_c =
\begin{bmatrix}
1 \\ -2
\end{bmatrix} \)
and \( P =
\begin{bmatrix}
3 & 1 \\
1 & 3
\end{bmatrix} \).

\makesubproblem{}{convex-optimization:problemSet1:6c}
Center \( \Bx_c =
\begin{bmatrix}
-2 \\ 1
\end{bmatrix} \)
and \( P =
\begin{bmatrix}
9 & -2 \\
-2 & 6
\end{bmatrix} \).

For each part (a)-(c) also compute each pair of eigenvalues and corresponding eigenvectors.

\makesubproblem{}{convex-optimization:problemSet1:6d}
Recall that the most geometrically meaningful property of the determinant of a square real matrix \(A\) is that its magnitude \( \Abs{\det A} \) is equal to the volume of the parallelepiped \( \calP \) formed by applying \( A \) to the unit cube \( \calC = \setlr{x|0 \le x \le 1} \).
(Recall that since \( \Bx \in \Rm{n} \) we interpret the inequalities coordinate-wise, i.e., \( 0 \le x_i \le 1 \) for all \( i = 1, \cdots, n \).)
In other words, if \( \calP = \setlr{ A \Bx| \Bx \in \calC} \) then \( \Abs{\det(A)}\) is equal to the volume of \( \calP\).
Furthermore, recall that the determinant of a matrix is zero if any of its eigenvalues are zero.
Explain how to interpret this latter fact in terms of the interpretation of \( \Abs{\det(A)} \) as the volume of \( \calP\).
} % makeproblem

\makeanswer{convex-optimization:problemSet1:6}{
\withproblemsetsParagraph{
\makeSubAnswer{}{convex-optimization:problemSet1:6a}

The eigenvalues were found to be \( \setlr{2,1} \) with respective eigenvectors

\begin{dmath}\label{eqn:ProblemSet1Problem6:20}
\begin{aligned}
\Be_1 &= \inv{\sqrt{2}}
\begin{bmatrix}
1 \\
-1
\end{bmatrix} \\
\Be_2 &= \inv{\sqrt{2}}
\begin{bmatrix}
1 \\
1
\end{bmatrix}.
\end{aligned}
\end{dmath}

The ellipsoid is plotted in \cref{fig:ps1p6a:ps1p6aFig2}.

\imageFigure{../figures/ece1505-convex-optimization/ps1p6aFig2}{}{fig:ps1p6a:ps1p6aFig2}{0.5}

\makeSubAnswer{}{convex-optimization:problemSet1:6b}

The eigenvalues were found to be \( \setlr{4,2} \) with respective eigenvectors

\begin{dmath}\label{eqn:ProblemSet1Problem6:40}
\begin{aligned}
\Be_1 &= \inv{\sqrt{2}}
\begin{bmatrix}
1 \\
1
\end{bmatrix} \\
\Be_2 &= \inv{\sqrt{2}}
\begin{bmatrix}
1 \\
-1
\end{bmatrix}.
\end{aligned}
\end{dmath}

The ellipsoid is plotted in \cref{fig:ps1p6b:ps1p6bFig3}.

\imageFigure{../figures/ece1505-convex-optimization/ps1p6bFig3}{}{fig:ps1p6b:ps1p6bFig3}{0.5}

\makeSubAnswer{}{convex-optimization:problemSet1:6c}

The eigenvalues were found to be \( \setlr{10,5} \) with respective eigenvectors

\begin{dmath}\label{eqn:ProblemSet1Problem6:60}
\begin{aligned}
\Be_1 &= \inv{\sqrt{5}}
\begin{bmatrix}
2 \\
-1
\end{bmatrix} \\
\Be_2 &= \inv{\sqrt{5}}
\begin{bmatrix}
1 \\
2
\end{bmatrix}.
\end{aligned}
\end{dmath}

The ellipsoid is plotted in \cref{fig:ps1p6c:ps1p6cFig4}.

\imageFigure{../figures/ece1505-convex-optimization/ps1p6cFig4}{}{fig:ps1p6c:ps1p6cFig4}{0.5}

\makeSubAnswer{}{convex-optimization:problemSet1:6d}

Some of the abstraction of the statement

\begin{equation}\label{eqn:ProblemSet1Problem6:80}
\calP = \setlr{ A \Bx| \Bx \in \calC }
\end{equation}

can be removed by expressing the matrix in terms of its columns \( A = [ \Ba_1 \Ba_2 \cdots \Ba_n ] \), and the vector in terms of coordinates \( \Bx = \sum x_i \Be_i\, x_i \in [0,1] \), so

\begin{equation}\label{eqn:ProblemSet1Problem6:100}
\calP = \setlr{ \sum \Ba_i x_i | x_i \in [0,1] },
\end{equation}

which shows that this set \( \calP \) is the span of the columns of the matrix \( A \) over the unit cube.  The span of the columns takes a particularly simple form if the linear transformation represented by the matrix is represented in a basis for which that transformation takes its
Jordan canonical form \citep{damiano1988course}.
This is a basis for which the matrix representation of the linear transformation is either completely diagonal, or in specific upper triangular form with zeros everywhere but the superdiagonal (and only ones or zeros on the superdiagonal).  Such a similarity transformation

\begin{dmath}\label{eqn:ProblemSet1Problem6:140}
A = E J E^{-1},
\end{dmath}

is determinant preserving, since

\begin{dmath}\label{eqn:ProblemSet1Problem6:160}
\Abs{ A }
= \Abs{ E } \Abs{ J } \Abs{ E^{-1} }
= \Abs{ E } \Abs{ J } \inv{ \Abs{E} }
= \Abs{ J }.
\end{dmath}

A couple examples are

\begin{dmath}\label{eqn:ProblemSet1Problem6:120}
\begin{bmatrix}
3 & 1 \\
0 & 3
\end{bmatrix}, \quad
\begin{bmatrix}
1 & 0 \\
0 & 2
\end{bmatrix}, \quad
\begin{bmatrix}
\epsilon & 0 & 0 \\
0        & 2 & 0 \\
0        & 0 & 3
\end{bmatrix}, \quad
\begin{bmatrix}
\epsilon & 0 & 0 \\
0        & 2 & 1 \\
0        & 0 & 2
\end{bmatrix},
\end{dmath}

for which the respective sets \( \calP \) are sketched in \cref{fig:ps1p6d:ps1p6dFig1}.

\imageFourFiguresTwoLines
{../figures/ece1505-convex-optimization/ps1p6dFig1}
{../figures/ece1505-convex-optimization/ps1p6dFig2}
{../figures/ece1505-convex-optimization/ps1p6dFig3}
{../figures/ece1505-convex-optimization/ps1p6dFig4}
{Parallelepiped volumes}{fig:ps1p6d:ps1p6dFig1}{scale=0.15}

Note that the parallelograms are altered by such a similarity transformation, but the (oriented area) given by the determinant is invariant.  This is illustrated in \cref{fig:ps1p6d:ps1p6dParallelogramsFig1} for

\begin{dmath}\label{eqn:ProblemSet1Problem6:180}
\begin{bmatrix}
3 & 1 \\
-1 & 1
\end{bmatrix}
=
\begin{bmatrix}
1 & 1 \\
-1 & 1
\end{bmatrix}
\begin{bmatrix}
1 & 1 \\
0 & 1
\end{bmatrix}
\begin{bmatrix}
\inv{2} & -\inv{2} \\
\inv{2} & \inv{2}
\end{bmatrix},
\end{dmath}

for which the oriented area given by the determinant is unity in both cases.

\imageFigure{../figures/ece1505-convex-optimization/ps1p6dParallelogramsFig1}{Parallelograms after similarity transformation}{fig:ps1p6d:ps1p6dParallelogramsFig1}{0.3}

In a basis for which the matrix has its Jordan form, if any one of the eigenvalues is zero, then the volume of the parallelepiped in that basis will be zero, since the parallelepiped cannot have any volume along that dimension.  In the two three dimensional examples above, one of the eigenvalues was picked to be \( \epsilon \), a quantity that can be made arbitrarily small.  When any eigenvector is decreased to zero in that matrix's Jordan basis representation, it is clear that the corresponding parallelepiped volume also goes to zero, as does the determinant (which is just the product of the diagonal elements in this representation).

In summary, if \( n - r \ne 0 \) is the number of zero eigenvalues of a matrix (where \( n \) is the dimension, and \( r \) is the rank), then
parallelepiped represented by the span of the columns of a matrix over the unit cube has no volume (and zero determinant), because
such a parallelepiped will have no height along \( n - r \) dimensions.
} % redaction
} % answer
