%
% Copyright © 2017 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%
\input{../latex/blogpost.tex}
\renewcommand{\basename}{convex-optimization2}
\renewcommand{\dirname}{notes/ece1505/}
\newcommand{\keywords}{ECE1505H}
\input{../latex/peeter_prologue_print2.tex}

\usepackage{ece1505}
\usepackage{peeters_braket}
%\usepackage{peeters_layout_exercise}
\usepackage{peeters_figures}
\usepackage{mathtools}
\usepackage{siunitx}
\usepackage{peeters_layout_exercise}

\beginArtNoToc
\generatetitle{ECE1505H Convex Optimization.  Lecture 2: Mathematical background.  Taught by Prof.\ Stark Draper}
%\chapter{Mathematical background}
\label{chap:convex-optimization2}

\paragraph{Disclaimer}

Peeter's lecture notes from class.  These may be incoherent and rough.

These are notes for the UofT course ECE1505H, Convex Optimization, taught by Prof. Stark Draper, covering \textchapref{{1}} \citep{boyd2004convex} content.

\paragraph{Topics}

\begin{itemize}
\item Calculus: Derivatives and Jacobians, Gradients, Hessians, approximation functions.
\item Linear algebra, Matrices, decompositions, ...
\end{itemize}

\section{Norms}
%\paragraph{Vector space}

\makedefinition{Vector space}{dfn:convex-optimizationLecture2:1}{
A set of elements (vectors) that is closed under vector addition and scaling.
} % definition

\makedefinition{Normed vector spaces}{dfn:convex-optimizationLecture2:2}{
A vector space with a notion of lenght of any signle vector, the ``norm''.
} % definition

\makedefinition{Inner product space.}{dfn:convex-optimizationLecture2:3}{
A normed vector space with a notion of a real angle vetween any pair of vectors.
} % definition

This course has a focus on optimization in \R{n}.  Complex spaces in the context of this course can be considered with a mapping \( \text{\C{n}} \rightarrow \Rm{2 n} \).

\makedefinition{Norm.}{dfn:convex-optimizationLecture2:4}{
A norm is a function operating on a vector

\begin{dmath*}
\Bx = ( x_1, x_2, \cdots, x_n )
\end{dmath*}

that provides a mapping

\begin{equation*}
\Norm{ \cdot } : \Rm{n} \rightarrow \bbR,
\end{equation*}

where

\begin{itemize}
\item \( \Norm{ \Bx } \ge 0 \)
\item \( \Norm{ \Bx } = 0 \qquad \iff \Bx = 0 \)
\item \( \Norm{ t \Bx } = \Abs{t} \Norm{ \Bx } \)
\item \( \Norm{ \Bx + \By } \le \Norm{ \Bx } + \Norm{\By} \).  This is the triangle inequality.
\end{itemize}
} % definition

\paragraph{Example: Euclidean norm}

\begin{dmath}\label{eqn:convex-optimizationLecture2:24}
\Norm{\Bx} = \sqrt{ \sum_{i = 1}^n x_i^2 }
\end{dmath}

\paragraph{Example: \(l_p\)-norms}

\begin{dmath}\label{eqn:convex-optimizationLecture2:44}
\Norm{\Bx}_p = \lr{ \sum_{i = 1}^n \Abs{x_i}^p }^{1/p}.
\end{dmath}

For \( p = 1 \), this is

\begin{dmath}\label{eqn:convex-optimizationLecture2:64}
\Norm{\Bx}_1 = \sum_{i = 1}^n \Abs{x_i},
\end{dmath}

For \( p = 2 \), this is the Euclidean norm \cref{eqn:convex-optimizationLecture2:24}.
For \( p = \infty \), this is

\begin{dmath}\label{eqn:convex-optimizationLecture2:324}
\Norm{\Bx}_\infty = \max_{i = 1}^n \Abs{x_i}.
\end{dmath}

\makedefinition{Unit ball}{dfn:convex-optimizationLecture2:10}{

\begin{dmath*}
\setlr{ \Bx | \Norm{\Bx} \le 1 }
\end{dmath*}

} % definition

The \( l_2 \) norm is not only familiar, but can be ``induced'' by an inner product

\begin{equation}\label{eqn:convex-optimizationLecture2:84}
\innerproduct{\Bx}{\By} = \Bx^\T \By = \sum_{i = 1}^n x_i y_i,
\end{equation}

which is not true for all norms.  The norm induced by this inner product is

\begin{dmath}\label{eqn:convex-optimizationLecture2:104}
\Norm{\Bx}_2 = \sqrt{ \innerproduct{\Bx}{\By} }
\end{dmath}

Inner product spaces have a notion of angle given by

\begin{dmath}\label{eqn:convex-optimizationLecture2:124}
\innerproduct{\Bx}{\By} = \Norm{\Bx} \Norm{\By} \cos \theta,
\end{dmath}

F3

and always satify the Cauchy-Schwartz inequality

\begin{dmath}\label{eqn:convex-optimizationLecture2:144}
\innerproduct{\Bx}{\By} \le \Norm{\Bx}_2 \Norm{\By}_2.
\end{dmath}

In an inner product space we say \( \Bx \) and \( \By \) are orthogonal vectors \( \Bx \perp \By \) if
\( \innerproduct{\Bx}{\By} = 0 \).

F4

\section{Dual norm}

\makedefinition{Dual norm}{dfn:convex-optimizationLecture2:20}{
Let \( \Norm{ \cdot } \) be a norm in \R{n}.  The ``dual'' norm \( \Norm{ \cdot }_\conj \) is defined as
\begin{equation*}
\Norm{\Bz}_\conj = \sup_\Bx \setlr{ \Bz^\T \Bx | \Norm{\Bx} \le 1 }.
\end{equation*}

where \( \sup \) is roughly the ``least upper bound''.
\index{sup}

This is a limit over the unit ball of \( \Norm{\cdot} \).
} % definition

\paragraph{\( l_2 \) dual}.

Dual of the \( l_2 \) is the \( l_2 \) norm.

F5

Proof:

\begin{dmath}\label{eqn:convex-optimizationLecture2:164}
\begin{aligned}
\Norm{\Bz}_\conj
&= \sup_\Bx \setlr{ \Bz^\T \Bx | \Norm{\Bx}_2 \le 1 } \\
&= \sup_\Bx \setlr{ \Norm{\Bz}_2 \Norm{\Bx}_2 \cos\theta | \Norm{\Bx}_2 \le 1 } \\
&\le \sup_\Bx \setlr{ \Norm{\Bz}_2 \Norm{\Bx}_2 | \Norm{\Bx}_2 \le 1 } \\
&\le
\cancel{\Norm{\Bz}_2}
\Norm{
\frac{\Bz}{ \cancel{\Norm{\Bz}_2} }
}_2 \\
&=
\Norm{\Bz}_2.
\end{aligned}
\end{dmath}

\paragraph{\( l_1 \) dual}.
For \( l_1 \), the dual is the \( l_\infty \) norm.  Proof:

\begin{equation}\label{eqn:convex-optimizationLecture2:184}
\Norm{\Bz}_\conj
=
\sup_\Bx \setlr{ \Bz^\T \Bx | \Norm{\Bx}_1 \le 1 },
\end{equation}

but
\begin{dmath}\label{eqn:convex-optimizationLecture2:204}
\Bz^\T \Bx
=
\sum_{i=1}^n z_i x_i \le
\Abs{
\sum_{i=1}^n z_i x_i
}
\le
\sum_{i=1}^n \Abs{z_i x_i },
\end{dmath}

so
\begin{dmath}\label{eqn:convex-optimizationLecture2:224}
\Norm{\Bz}_\conj
=
\sum_{i=1}^n \Abs{z_i}\Abs{ x_i }
\le \lr{ \max_{j=1}^n \Abs{z_j} }
\sum_{i=1}^n \Abs{ x_i }
\le \lr{ \max_{j=1}^n \Abs{z_j} }
=
\Norm{\Bz}_\infty.
\end{dmath}

F6

\paragraph{\( l_\infty \) dual}.

F7
\begin{equation}\label{eqn:convex-optimizationLecture2:244}
\Norm{\Bz}_\conj
=
\sup_\Bx \setlr{ \Bz^\T \Bx | \Norm{\Bx}_\infty \le 1 }.
\end{equation}

Here
\begin{dmath}\label{eqn:convex-optimizationLecture2:264}
\Bz^\T \Bx
=
\sum_{i=1}^n z_i x_i
\le
\sum_{i=1}^n \Abs{z_i}\Abs{ x_i }
\le
\lr{ \max_j \Abs{ x_j } }
\sum_{i=1}^n \Abs{z_i}
=
\Norm{\Bx}_\infty
\sum_{i=1}^n \Abs{z_i}.
\end{dmath}

So
\begin{equation}\label{eqn:convex-optimizationLecture2:284}
\Norm{\Bz}_\conj
\le
\sum_{i=1}^n \Abs{z_i}
=
\Norm{\Bz}_1.
\end{equation}

Statement from the lecture: I'm not sure where this fits:

\begin{dmath}\label{eqn:convex-optimizationLecture2:304}
x_i^\conj
=
\left\{
\begin{array}{l l}
+1 & \quad \mbox{\( z_i \ge 0 \)} \\
-1 & \quad \mbox{\( z_i \le 0 \)}
\end{array}
\right.
\end{dmath}

%\subsection{Calculus}

\input{jacobianAndHessian.tex}

%\subsection{Original Calculus lecture notes.}
%\subsubsection{Gradient}
%
%Consider a scalar function
%
%\begin{equation}\label{eqn:convex-optimizationLecture2:344}
%F: \Rm{n} \rightarrow \bbR
%\end{equation}
%
%where for \( \Bx \in \Rm{n} \)
%
%\begin{dmath}\label{eqn:convex-optimizationLecture2:364}
%\spacegrad F(\Bx)
%=
%\begin{bmatrix}
%\PD{x_1}{F(\Bx)} \\
%\PD{x_2}{F(\Bx)} \\
%\vdots \\
%\PD{x_n}{F(\Bx)}
%\end{bmatrix}
%\end{dmath}
%

%\subsubsection{Second derivative.}
%
%The second derivative for \( \Rm{n} \rightarrow \bbR \), the ``Hessian'' is
%
%\begin{dmath}\label{eqn:convex-optimizationLecture2:844}
%\spacegrad^2 F =
%\begin{bmatrix}
%\frac{\partial^2 F}{\partial x_i \partial x_j }
%\end{bmatrix}
%=
%\begin{bmatrix}
%\frac{\partial^2 F}{\partial x_1^2 } & \frac{\partial^2 F}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 F}{\partial x_1 \partial x_n} \\
%\frac{\partial^2 F}{\partial x_2 \partial x_1 } & \frac{\partial^2 F}{\partial x_2^2 } & \cdots & \frac{\partial^2 F}{\partial x_2 \partial x_n} \\
%\vdots \\
%\end{bmatrix}
%\end{dmath}
%
\EndArticle
%\EndNoBibArticle
