%
% Copyright © 2017 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%
%\chapter{Local vs. Global, and composition of functions}
%%%\label{chap:convexOptimization8}
%%%
%%%\section{Disclaimer}
%%%
%%%Peeter's lecture notes from class.  These may be incoherent and rough.
%%%
%%%These are notes for the UofT course ECE1505H, Convex Optimization, taught by Prof. Stark Draper, from \citep{boyd2004convex}.
%%%
%%%\section{Today}
%%%
%%%\begin{itemize}
%%%\item Finish local vs global.
%%%\item Compositions of functions.
%%%\item Introduction to convex optimization problems.
%%%\end{itemize}
%%%
%\section{Continuing proof:}

Now we want to prove that if

\begin{equation*}
\begin{aligned}
\spacegrad F(\Bx^\conj) &= 0 \\
\spacegrad^2 F(\Bx^\conj) \ge 0
\end{aligned},
\end{equation*}

then \( \Bx^\conj\) is a local optimum.

Proof:

Again, using Taylor approximation

\begin{dmath}\label{eqn:convexOptimizationLecture8:20}
F(\Bx^\conj + \Bv) = F(\Bx^\conj) + \lr{ \spacegrad F(\Bx^\conj)}^\T \Bv + \inv{2} \Bv^\T \spacegrad^2 F(\Bx^\conj) \Bv + o(\Norm{\Bv}^2)
\end{dmath}

The linear term is zero by assumption, whereas the Hessian term is given as \( > 0 \).  Any direction that you move in, if your move is small enough, this is going uphill at a local optimum.

\section{Summarize:}

For twice continuously differentiable functions, at a local optimum \( \Bx^\conj \), then

\begin{dmath}\label{eqn:convexOptimizationLecture8:40}
\begin{aligned}
\spacegrad F(\Bx^\conj) &= 0 \\
\spacegrad^2 F(\Bx^\conj) \ge 0
\end{aligned}
\end{dmath}

If, in addition, \( F \) is convex, then \( \spacegrad F(\Bx^\conj) = 0 \) implies that \( \Bx^\conj \) is a global optimum.  i.e. for (unconstrained) convex functions, local and global optimums are equivalent.

\begin{itemize}
\item It is possible that a convex function does not have a global optimum.  Examples are \( F(x) = e^x \)
(\cref{fig:l8exponential:l8exponentialFig1})
, which has an \( \inf \), but no lowest point.

\imageFigure{../figures/ece1505-convex-optimization/l8exponentialFig1}{Exponential has no global optimum.}{fig:l8exponential:l8exponentialFig1}{0.2}

\item Our discussion has been for unconstrained functions.  For constrained problems (next topic) is not not necessarily true that \( \spacegrad F(\Bx) = 0 \) implies that \( \Bx \) is a global optimum, even for \( F \) convex.

As an example of a constrained problem consider

\begin{dmath}\label{eqn:convexOptimizationLecture8:n}
\begin{aligned}
\min &2 x^2 + y^2 \\
x &\ge 3 \\
y &\ge 5.
\end{aligned}
\end{dmath}

The level sets of this objective function are plotted in \cref{fig:l8constrainedMin:l8constrainedMinFig2}.  The optimal point is at \( \Bx^\conj = (3,5) \), where \( \spacegrad F \ne 0 \).

\imageFigure{../figures/ece1505-convex-optimization/l8constrainedMinFig2}{Constrained problem with optimum not at the zero gradient point.}{fig:l8constrainedMin:l8constrainedMinFig2}{0.3}
\end{itemize}

%\EndArticle
